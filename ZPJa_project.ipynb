{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e85fbc-725a-4ce2-96b3-ecfce144a575",
   "metadata": {},
   "source": [
    "# Import dependencies and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "721c1ece-becd-4874-8ef4-e5acafab13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "from json_repair import repair_json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import heapq\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de500f73-397d-490f-9d16-808ebba458eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06072fb9-ceec-4177-bc0f-db867f732f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f103d3b5ea2343ff883e138074a32ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186c8e65352e400185d82f98898c5368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973d0055a9a84b7887cb60080e40ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"pirxus/spokenwoz-whisper\")\n",
    "dataset = dataset.remove_columns(\"audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fdf50c8-cd7b-4766-ad55-860b7b10b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['wav_id', 'turn_index', 'text', 'agent_text', 'domains', 'slots', 'context'],\n",
      "        num_rows: 73950\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['wav_id', 'turn_index', 'text', 'agent_text', 'domains', 'slots', 'context'],\n",
      "        num_rows: 9104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['wav_id', 'turn_index', 'text', 'agent_text', 'domains', 'slots', 'context'],\n",
      "        num_rows: 17652\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wav_id': 'MUL0003',\n",
       " 'turn_index': 10,\n",
       " 'text': 'I will go there on Friday.',\n",
       " 'agent_text': 'and most of the time do like to arrive there.',\n",
       " 'domains': \"['restaurant']\",\n",
       " 'slots': \"{'restaurant': {'day': 'Friday', 'people': '1', 'area': 'centre', 'food': 'Indian'}}\",\n",
       " 'context': {'turn_index': [0, 2, 4, 6, 8],\n",
       "  'text': ['Hello, is this Customer Service Center?',\n",
       "   \"Well, I'm looking for a place to dine. Do you have any recommendation for me?\",\n",
       "   \"Well, I'd like to stay in the place with convenient transportation.\",\n",
       "   \"I'd like to try some Indian food.\",\n",
       "   'Yes, please. Book a table for one people.'],\n",
       "  'agent_text': ['Yes, this is Cosmos Service Center. How may I help?',\n",
       "   'Of course, there are plenty of restaurants. Do you have any specific area?',\n",
       "   'Okay, do you have any specific food type?',\n",
       "   'Let me check it for you. Yes, we got you a panel located in the center of the city, could meet your requirement. Do you want to book table?',\n",
       "   'Okay, and what day would you like to have your meal?'],\n",
       "  'domains': ['[]',\n",
       "   '[]',\n",
       "   \"['restaurant']\",\n",
       "   \"['restaurant']\",\n",
       "   \"['restaurant']\"],\n",
       "  'slots': ['{}',\n",
       "   '{}',\n",
       "   \"{'restaurant': {'area': 'centre'}}\",\n",
       "   \"{'restaurant': {'area': 'centre', 'food': 'Indian'}}\",\n",
       "   \"{'restaurant': {'people': '1', 'area': 'centre', 'food': 'Indian'}}\"]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "dataset['train'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66b621-63ba-4c68-a011-eb06b463b1c0",
   "metadata": {},
   "source": [
    "# Sentence embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90aa0dc-9086-4fe8-b9b5-8d7ddd01b442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0d3933df844349926c52bd77c7a082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2103daa99ba24cbd9fde026d5402ff4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a9f8da9da64f34ab6dba9021038584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508bff7776214dddb10435b0879b6fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736b0589a041474fb1680ed2f7fe6adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fdeb44dcd54a4387d34d3ee363884e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f30739b94b435cae446b10528e3050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbd06978b894d98be18810d41a2c9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32b257b6b334544bbdd102ce7ef4dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b185902e90f47648771631fa6b7e58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcf2e503a594bb5baf6624ec981f851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa4296e6de4c9bbc16e0e6cc0b797b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183246234fb9443ab979b5e4fac038d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder_d2f = SentenceTransformer('sergioburdisso/dialog2flow-joint-bert-base')\n",
    "embedder_labse = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e74cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue(sample, only_user=False, include_state=True):\n",
    "    dialogue = \"\"\n",
    "    for j in range(len(sample[\"context\"][\"text\"])):\n",
    "        if only_user:\n",
    "            dialogue += \" \" + sample[\"context\"][\"text\"][j]\n",
    "        else:\n",
    "            dialogue += \" User: \" + sample[\"context\"][\"text\"][j]\n",
    "            dialogue += \" Agent: \" + sample[\"context\"][\"agent_text\"][j]\n",
    "    if not only_user:\n",
    "        dialogue += \" User:\"\n",
    "    dialogue += \" \" + sample[\"text\"]\n",
    "    if include_state:\n",
    "        dialogue += \" Domains: \" + sample[\"domains\"]\n",
    "        dialogue += \" Slots: \" + sample[\"slots\"]\n",
    "    return dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7019b047-b9ff-4cdf-825b-b83a843a232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D2F embedded dataset\n",
      "Loading LaBSE embedded dataset\n"
     ]
    }
   ],
   "source": [
    "def embed_dataset(embedder, dataset, path, batch_size=128):\n",
    "    #If the dataset was already embedded, load it from a file to save time\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    embedded_rows = [\n",
    "    {\n",
    "        'wav_id': sample['wav_id'],\n",
    "        'turn_index': sample['turn_index'],\n",
    "        'embedding': embedding\n",
    "    }\n",
    "    for batch_start in range(0, len(dataset), batch_size)\n",
    "    for sample, embedding in zip(\n",
    "        dataset[batch_start:batch_start+batch_size],\n",
    "        embedder.encode(\n",
    "            [extract_dialogue(sample) for sample in dataset[batch_start:batch_start+batch_size]],\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "    )]\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(embedded_rows, f)\n",
    "    return embedded_rows \n",
    "\n",
    "print(\"Loading D2F embedded dataset...\")\n",
    "embedded_train_d2f = embed_dataset(embedder_d2f, list(dataset['train'])[:512], \"embedded_train.d2f\")\n",
    "print(\"Loading LaBSE embedded dataset...\")\n",
    "embedded_train_labse = embed_dataset(embedder_labse, list(dataset['train'])[:512], \"embedded_train.labse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19ba9bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_k_most_similar(sample, k, embedder, dataset, embedded_dataset):\n",
    "    sample_emb = embedder.encode(extract_dialogue(sample))\n",
    "    \n",
    "    M = np.stack([row['embedding'] for row in embedded_dataset])\n",
    "    s = np.array(sample_emb)\n",
    "    \n",
    "    dists = np.linalg.norm(M - s, axis=1) # L2 distance\n",
    "    dists[dists == 0] = np.inf # Ignore exact match\n",
    "\n",
    "    idx = np.argpartition(dists, k)[:k]\n",
    "    idx = idx[np.argsort(dists[idx])]\n",
    "\n",
    "    samples   = [dataset[i] for i in idx]\n",
    "    distances = dists[idx].tolist()\n",
    "    return samples, distances\n",
    "\n",
    "#find_k_most_similar(dataset['train'][120], 3, embedder_d2f, dataset['train'], embedded_train_d2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745661b",
   "metadata": {},
   "source": [
    "# Prompt creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db4f75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample, k, embedder_name, only_user=False):\n",
    "    assert(embedder_name == \"D2F\" or embedder_name == \"LaBSE\")\n",
    "    if embedder_name == \"D2F\":\n",
    "        embedder = embedder_d2f\n",
    "        embedded_dataset = embedded_train_d2f\n",
    "    else:\n",
    "        embedder = embedder_labse\n",
    "        embedded_dataset = embedded_train_labse\n",
    "        \n",
    "    (demonstrations, _) = find_k_most_similar(sample, k, embedder, dataset['train'], embedded_dataset)\n",
    "    prompt = 'Instruction: Identify the slot keys and values.\\n'\n",
    "    for i in range(k):\n",
    "        prompt += extract_dialogue(demonstrations[k-i-1], only_user, include_state=True) + \"\\n\"\n",
    "    #New sample\n",
    "    prompt += extract_dialogue(sample, only_user, include_state=False)\n",
    "    prompt += \" Domains: \" + sample[\"domains\"]\n",
    "    prompt += \" Slots: \"\n",
    "    return prompt\n",
    "    \n",
    "#create_prompt(dataset['train'][120], 3, \"D2F\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d0aea-dff5-4f15-9b5f-62993549c130",
   "metadata": {},
   "source": [
    "# LLM and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b92359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad506b9fdc040728025ca250f1e32ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2385eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Identify the slot keys and values.\n",
      " Hello? Yes, I'm looking for the information in Cambridge. I'm looking for a restaurant. I suppose the restaurant should be in the expensive price range. I will travel in the eastern part of the city, so I prefer to eat there. That's right. Do you have any recommendation of the restaurant? Okay. Okay, Mano. Okay, I suppose I would like to try the second one. No, but I need some other information. My know their post code Go ahead. Thank you so much. May I know the address? God, Ed, thank you very much. Also, I'm looking for a place to stay. Well, I suppose the hotel should be in the type of guest house. And I suppose we should be in the same area of the restaurant. Also, I suppose it should have a style of floor. Domains: ['hotel', 'restaurant'] Slots: {'hotel': {'area': 'East', 'stars': '4', 'type': 'guest house'}, 'restaurant': {'area': 'East', 'name': 'Royal Standard'}}\n",
      " Hello? Yes, I'm looking for the information in Cambridge. I'm looking for a restaurant. I suppose the restaurant should be in the expensive price range. I will travel in the eastern part of the city, so I prefer to eat there. That's right. Do you have any recommendation of the restaurant? Okay. Okay, Mano. Okay, I suppose I would like to try the second one. No, but I need some other information. My know their post code Go ahead. Thank you so much. May I know the address? God, Ed, thank you very much. Also, I'm looking for a place to stay. Well, I suppose the hotel should be in the type of guest house. Domains: ['hotel', 'restaurant'] Slots: {'hotel': {'type': 'guest house'}, 'restaurant': {'area': 'East', 'name': 'Royal Standard'}}\n",
      " Hi. Hi, I need to stay overnight in Cambridge. Please find me a hotel. I'd like to find an expensive one. Domains: ['hotel'] Slots: {'hotel': {'pricerange': 'expensive', 'type': 'hotel'}}\n",
      " Hi. Hi, I need to stay overnight in Cambridge. Please find me a hotel. I'd like to find an expensive one. And I need free internet and parking lots, please. Domains: ['hotel'] Slots: \n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][100]\n",
    "prompt = create_prompt(sample, 3, \"D2F\", True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9c7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_output(output):\n",
    "    try:\n",
    "        obj = repair_json(output)\n",
    "        return obj\n",
    "    except Exception as e:\n",
    "        print(\"Exception during json repair: \", e)\n",
    "        return \"\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9f56666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"hotel\": {\"pricerange\": \"expensive\", \"internet\": \"free\", \"parking\": \"lots\"}}\n",
      "Ground truth:  {'hotel': {'internet': 'yes', 'parking': 'yes', 'pricerange': 'expensive', 'type': 'hotel'}}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "output_string = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "print(repair_output(output_string))\n",
    "print(\"Ground truth: \", sample[\"slots\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6155e4e",
   "metadata": {},
   "source": [
    "# Metrics and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a6d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_goal_accuracy(results, ground_truths):\n",
    "    return 0.4\n",
    "\n",
    "def slot_error_rate(results, ground_truths):\n",
    "    return 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b86070e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_utterances(samples, k, embedder_name, device):\n",
    "    results = [[], []]\n",
    "    for sample in tqdm.tqdm(samples, desc=\"Processing samples\"):\n",
    "        prompts = [create_prompt(sample, k, embedder_name, only_user=True), create_prompt(sample, k, embedder_name, only_user=False)]\n",
    "        for i in tqdm.tqdm(range(len(prompts)), desc=\"Processing prompts\", leave=False):\n",
    "            inputs = tokenizer(prompts[i], return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "            result = repair_output(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
    "            results[i].append(result)\n",
    "    return results\n",
    "\n",
    "def test_model_demonstrations(samples, ks, only_user, embedder_name, device):\n",
    "    results = [[] for k in ks]\n",
    "    for sample in tqdm.tqdm(samples, desc=\"Processing samples\"):\n",
    "        prompts = [create_prompt(sample, k, embedder_name, only_user=only_user) for k in ks]\n",
    "        for i in tqdm.tqdm(range(len(prompts)), desc=\"Processing prompts\", leave=False):\n",
    "            inputs = tokenizer(prompts[i], return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "            result = repair_output(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
    "            results[i].append(result)\n",
    "    return results\n",
    "\n",
    "def test_model_demonstrations(samples, k, only_user, device):\n",
    "    results = [[], []]\n",
    "    for sample in tqdm.tqdm(samples, desc=\"Processing samples\"):\n",
    "        prompts = [create_prompt(sample, k, embedder_name=\"D2F\", only_user=only_user), create_prompt(sample, k, embedder_name=\"LaBSE\", only_user=only_user)]\n",
    "        for i in tqdm.tqdm(range(len(prompts)), desc=\"Processing prompts\", leave=False):\n",
    "            inputs = tokenizer(prompts[i], return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "            result = repair_output(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
    "            results[i].append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6ccbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "def evaluate_model(dataset, n, device, test_type):\n",
    "    assert(test_type == \"utterances\" or test_type == \"demonstrations\" or test_type == \"embedder\")\n",
    "    print(\"Selecting samples...\")\n",
    "    positions = random.sample(range(len(dataset)), n)\n",
    "    samples = [dataset[p] for p in positions]\n",
    "    print(\"Testing model...\")\n",
    "    if test_type == \"utterances\":\n",
    "        results = test_model_utterances(samples, k=3, embedder_name=\"D2F\", device=device)\n",
    "    elif test_type == \"demonstrations\":\n",
    "        results = test_model_demonstrations(samples, k=[1, 3, 5, 10], only_user=True, embedder_name=\"D2F\", device=device)\n",
    "    elif test_type == \"demonstrations\":\n",
    "        results = test_model_embedder(samples, k=3, only_user = True, device=device)\n",
    "    gts = [sample[\"slots\"] for sample in samples]\n",
    "    print(\"Evaluating joint goal accuracies...\")\n",
    "    jgas = [joint_goal_accuracy(r, gts) for r in results]\n",
    "    print(\"Evaluating slot error rates...\")\n",
    "    sers = [slot_error_rate(r, gts) for r in results]\n",
    "    return jgas, sers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6b5f42e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting samples...\n",
      "Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|                                                                        | 0/1 [00:00<?, ?it/s]\n",
      "Processing prompts:   0%|                                                                        | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing prompts:  50%|███████████████████████████████▌                               | 1/2 [03:14<03:14, 194.32s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing prompts: 100%|███████████████████████████████████████████████████████████████| 2/2 [05:33<00:00, 162.17s/it]\u001b[A\n",
      "Processing samples: 100%|███████████████████████████████████████████████████████████████| 1/1 [05:58<00:00, 358.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating joint goal accuracies...\n",
      "Evaluating slot error rates...\n",
      "Utterances results: \n",
      "[0.4, 0.4]\n",
      "[0.3, 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jgas_utterances, sers_utterances = evaluate_model(dataset['test'], 2, device, test_type=\"utterances\")\n",
    "#jgas_demonstrations, sers_demonstrations = evaluate_model(dataset['test'], 1, device, test_type=\"demonstrations\")\n",
    "#jgas_embedder, sers_embedder = evaluate_model(dataset['test'], 1, device, test_type=\"embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d57fd-87b5-4a14-8cc7-a153f1e84530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Utterances results: \")\n",
    "print(jgas_utterances)\n",
    "print(sers_utterances)\n",
    "#print(\"Demonstrations results: \")\n",
    "#print(jgas_demonstrations)\n",
    "#print(sers_demonstrations)\n",
    "#print(\"Embedder results: \")\n",
    "#print(jgas_embedder)\n",
    "#print(sers_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c585d",
   "metadata": {},
   "source": [
    "Tomiinek/MultiWOZ_Evaluation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8039c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zpja2",
   "language": "python",
   "name": "zpja2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
